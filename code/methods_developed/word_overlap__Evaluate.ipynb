{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word overlap_ Evaluate.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_Jf_xC6DtGQ",
        "outputId": "4c470538-179f-405c-de1c-79f13b7be5f2"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vbcf4mPCXQph",
        "outputId": "99e5d9b3-fd27-4904-e61f-49caf53b7fbb"
      },
      "source": [
        "!pip install google_trans_new"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting google_trans_new\n",
            "  Downloading https://files.pythonhosted.org/packages/f9/7b/9f136106dc5824dc98185c97991d3cd9b53e70a197154dd49f7b899128f6/google_trans_new-1.1.9-py3-none-any.whl\n",
            "Installing collected packages: google-trans-new\n",
            "Successfully installed google-trans-new-1.1.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfO61jIH5Gm0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e5470ad-dc2c-4a80-ba29-d1cffc0c9588"
      },
      "source": [
        "import io\n",
        "import numpy as np\n",
        "from google_trans_new import google_translator\n",
        "import nltk\n",
        "nltk.download('stopwords') \n",
        "import string\n",
        "from itertools import groupby \n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTNhw1QZ1DgX"
      },
      "source": [
        "### Importing data from json file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFPN0nyj7pA8"
      },
      "source": [
        "import json \n",
        "  \n",
        "# Opening JSON file \n",
        "f = open('/content/drive/My Drive/test_data_v2.json',) \n",
        "  \n",
        "# returns JSON object as  \n",
        "# a dictionary \n",
        "data = json.load(f) \n",
        "  \n",
        "# Iterating through the json \n",
        "# list \n",
        "d = {}\n",
        "d = data\n",
        "  \n",
        "# Closing file \n",
        "f.close() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsxQO3dFvfTE"
      },
      "source": [
        "### Getting a_c_p for getting all the triples\n",
        "# Opening JSON file \n",
        "f = open('/content/drive/My Drive/a_c_p.json',) \n",
        "data = json.load(f) \n",
        "acp = {}\n",
        "acp = data\n",
        "f.close() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b8kW8qj1KCt"
      },
      "source": [
        "### Storing sentences and triples from one article\n",
        "Here we store all the sentences and the triples from one article. The article is about the actress 'Kalpana'. We display 5 sentences and 5 triples corresponding to her here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Rh8jjCs5cCz"
      },
      "source": [
        "\n",
        "def load_vec(emb_path, nmax=50000):\n",
        "    vectors = []\n",
        "    word2id = {}\n",
        "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
        "        next(f)\n",
        "        for i, line in enumerate(f):\n",
        "            word, vect = line.rstrip().split(' ', 1)\n",
        "            vect = np.fromstring(vect, sep=' ')\n",
        "            assert word not in word2id, 'word found twice'\n",
        "            vectors.append(vect)\n",
        "            word2id[word] = len(word2id)\n",
        "            if len(word2id) == nmax:\n",
        "                break\n",
        "    id2word = {v: k for k, v in word2id.items()}\n",
        "    embeddings = np.vstack(vectors)\n",
        "    return embeddings, id2word, word2id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtoLKVcW3u_7"
      },
      "source": [
        "### Getting source and target embeddings from the aligned multilingual vector space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69tEieKr5fs3"
      },
      "source": [
        "dir = '/content/drive/My Drive/Algo Name detection implementation/Text/'\n",
        "src_path = '/content/drive/My Drive/wiki.hi.align.vec'\n",
        "tgt_path = '/content/drive/My Drive/wiki.en.align.vec'\n",
        "nmax = 50000  # maximum number of word embeddings to load\n",
        "\n",
        "src_embeddings, src_id2word, src_word2id = load_vec(src_path, nmax)\n",
        "tgt_embeddings, tgt_id2word, tgt_word2id = load_vec(tgt_path, nmax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTUW3Tba6JrL"
      },
      "source": [
        "**Get Nearest Neighbours**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2167UAg6TLk"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def get_nn(word, src_emb, src_id2word, tgt_emb, tgt_id2word, K=5):\n",
        "    # print(\"Nearest neighbors of \\\"%s\\\":\" % word)\n",
        "    word2id = {v: k for k, v in src_id2word.items()}\n",
        "    targetwordlist = []                               # List of target words for the source word \n",
        "    if word in word2id:                               #Check if word is in vocab\n",
        "      word_emb = src_emb[word2id[word]]\n",
        "      scores = (tgt_emb / np.linalg.norm(tgt_emb, 2, 1)[:, None]).dot(word_emb / np.linalg.norm(word_emb))\n",
        "      k_best = scores.argsort()[-K:][::-1]\n",
        "      for i, idx in enumerate(k_best):\n",
        "          # print((scores[idx], tgt_id2word[idx]))      #To give both distance and word\n",
        "          if tgt_id2word[idx] not in stop_words:\n",
        "            targetwordlist.append(tgt_id2word[idx].lower())\n",
        "      return targetwordlist\n",
        "    else:\n",
        "      translator = google_translator()\n",
        "      translate_text = translator.translate(word,lang_tgt='en') \n",
        "      transw = translate_text\n",
        "      if transw not in stop_words:\n",
        "      # print(word,\" - Translated - \",transw)\n",
        "        return [transw.lower()]\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2NAioDT5Mn2"
      },
      "source": [
        "These are the 5 nearest neighbours in English for the sample word - अभिनेता. We obtain these nearest neighbours from the aligned multilingual vector space. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIcCqGnE6clC",
        "outputId": "c4ea96e2-337b-48ea-96b7-6f6dcc4901f8"
      },
      "source": [
        "# printing nearest neighbors in the target space\n",
        "src_word = 'अभिनेता'\n",
        "l = get_nn(src_word, src_embeddings, src_id2word, tgt_embeddings, tgt_id2word, K=5)\n",
        "print(\"Top 5 nearest neighbours for the word : \",src_word)\n",
        "for i,e in enumerate(l):\n",
        "  print(\"\\n\",i+1,\". \",e)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 5 nearest neighbours for the word :  अभिनेता\n",
            "\n",
            " 1 .  actor\n",
            "\n",
            " 2 .  actors\n",
            "\n",
            " 3 .  actress\n",
            "\n",
            " 4 .  actresses\n",
            "\n",
            " 5 .  film\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vuyn1H8q39HU"
      },
      "source": [
        "**Function for word overlap**\n",
        "We simply return the number of words in common between two phrases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-LDbFsrafdq",
        "outputId": "339bd236-57db-47bc-9acd-342716e0a442"
      },
      "source": [
        "def wordoverlap(entity_text,sentence_text_list):\n",
        "  words = entity_text.split(' ')    #splitting the predicate's or object's words\n",
        "  score = 0                         # calculate no of word matches\n",
        "  for w in words:\n",
        "    if w.lower() in sentence_text_list:\n",
        "      score = score+1\n",
        "  return score\n",
        "print(\"Word overlap score for the phrases : 'cause of death' and 'actress died due to a bad cause of death' = \",wordoverlap(\"cause of death\",\"actress died due to a bad cause of death\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word overlap score for the phrases : 'cause of death' and 'actress died due to a bad cause of death' =  3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ty5d4ZFd7UaW"
      },
      "source": [
        "### Matching triples with sentences\n",
        "We find 5 English nearest neighbours for each word in the Hindi sentence. Then , we put all these english words in a list and find the total word overlap with the triples. We keep a threshold for a minimum word overlap score required to match a triple with the sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWSa6JZFp9Yd"
      },
      "source": [
        "actors_test,cricketers_test,politicians_test = d['actors'], d['cricketers'], d['politicians']\n",
        "actors, cricketers,politicians = actors_test,cricketers_test,politicians_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7Dhw4jLp-sG"
      },
      "source": [
        "# Creating the dictionary for the test data where key = sentence and value = list of matching triples\n",
        "actors_test_dict, cricketers_test_dict,politicians_test_dict = {},{},{}\n",
        "for l in actors_test:\n",
        "  for k,v in l.items():\n",
        "    if k == 'sentence':\n",
        "      sentence = v\n",
        "    if k == 'triples':\n",
        "      triple_list = v\n",
        "  t = [(e['subject'],e['predicate'],e['object']) for e in triple_list]\n",
        "  actors_test_dict[sentence] = set(t)\n",
        "\n",
        "for l in cricketers_test:\n",
        "  for k,v in l.items():\n",
        "    if k == 'sentence':\n",
        "      sentence = v\n",
        "    if k == 'triples':\n",
        "      triple_list = v\n",
        "  t = [(e['subject'],e['predicate'],e['object']) for e in triple_list]\n",
        "  cricketers_test_dict[sentence] = set(t)\n",
        "\n",
        "for l in politicians_test:\n",
        "  for k,v in l.items():\n",
        "    if k == 'sentence':\n",
        "      sentence = v\n",
        "    if k == 'triples':\n",
        "      triple_list = v\n",
        "  t = [(e['subject'],e['predicate'],e['object']) for e in triple_list]\n",
        "  politicians_test_dict[sentence] = set(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atzUG7NbrNJF",
        "outputId": "2f01320f-d3f3-4807-bc55-0c3ad65c875d"
      },
      "source": [
        "len(actors_test_dict),len(cricketers_test_dict),len(politicians_test_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60, 13, 30)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNDf_dGhv3AL"
      },
      "source": [
        "### We had got a_c_p.json. We keep only the relevant triples by filtering by entity id in test annotated data\n",
        "actors_trip,cricketers_trip,politician_trip = [],[],[]\n",
        "actors_sent,cricketers_sent,politician_sent = [],[],[]\n",
        "\n",
        "# Putting actors,cricketers and politicians from a_c_p\n",
        "act,cric,pol = acp['a'],acp['c'],acp['p']\n",
        "\n",
        "############### Actors #######################\n",
        "\n",
        "# Iterating over test annotated data and keeping relevant triples only\n",
        "entity_tracking = []\n",
        "for e in actors_test:\n",
        "  eid = e['entity_id']\n",
        "  for ele in act:\n",
        "    if eid == ele and eid not in entity_tracking:\n",
        "      entity_tracking.append(eid)\n",
        "      triples = act[ele]['triples']       #Getting triples for the matching entity id\n",
        "      subject = act[ele]['personLabel']\n",
        "      triplist = []\n",
        "      for trip in triples:\n",
        "        predicate = trip['propertyLabel']\n",
        "        obj = trip['objectLabel']\n",
        "        trip_tuple = (subject,predicate,obj)\n",
        "        triplist.append(trip_tuple)\n",
        "      actors_trip.append(triplist)\n",
        "\n",
        "# Iterating over test annotated data and grouping annotated sentences together by entity id\n",
        "\n",
        "for eid in entity_tracking:\n",
        "  sentence_list = []\n",
        "  for e in actors_test:\n",
        "    if eid == e['entity_id']:\n",
        "      sentence = e['sentence']\n",
        "      sentence_list.append(sentence)\n",
        "  actors_sent.append(sentence_list)\n",
        "\n",
        "\n",
        "############### Cricketers #######################\n",
        "\n",
        "# Iterating over test annotated data and keeping relevant triples only\n",
        "entity_tracking = []\n",
        "for e in cricketers_test:\n",
        "  eid = e['entity_id']\n",
        "  for ele in cric:\n",
        "    if eid == ele and eid not in entity_tracking:\n",
        "      entity_tracking.append(eid)\n",
        "      triples = cric[ele]['triples']       #Getting triples for the matching entity id\n",
        "      subject = cric[ele]['personLabel']\n",
        "      triplist = []\n",
        "      for trip in triples:\n",
        "        predicate = trip['propertyLabel']\n",
        "        obj = trip['objectLabel']\n",
        "        trip_tuple = (subject,predicate,obj)\n",
        "        triplist.append(trip_tuple)\n",
        "      cricketers_trip.append(triplist)\n",
        "\n",
        "# Iterating over test annotated data and grouping annotated sentences together by entity id\n",
        "\n",
        "for eid in entity_tracking:\n",
        "  sentence_list = []\n",
        "  for e in cricketers_test:\n",
        "    if eid == e['entity_id']:\n",
        "      sentence = e['sentence']\n",
        "      sentence_list.append(sentence)\n",
        "  cricketers_sent.append(sentence_list)\n",
        "\n",
        "\n",
        "############### Politicians #######################\n",
        "\n",
        "# Iterating over test annotated data and keeping relevant triples only\n",
        "entity_tracking = []\n",
        "for e in politicians_test:\n",
        "  eid = e['entity_id']\n",
        "  for ele in pol:\n",
        "    if eid == ele and eid not in entity_tracking:\n",
        "      entity_tracking.append(eid)\n",
        "      triples = pol[ele]['triples']       #Getting triples for the matching entity id\n",
        "      subject = pol[ele]['personLabel']\n",
        "      triplist = []\n",
        "      for trip in triples:\n",
        "        predicate = trip['propertyLabel']\n",
        "        obj = trip['objectLabel']\n",
        "        trip_tuple = (subject,predicate,obj)\n",
        "        triplist.append(trip_tuple)\n",
        "      politician_trip.append(triplist)\n",
        "\n",
        "# Iterating over test annotated data and grouping annotated sentences together by entity id\n",
        "\n",
        "for eid in entity_tracking:\n",
        "  sentence_list = []\n",
        "  for e in politicians_test:\n",
        "    if eid == e['entity_id']:\n",
        "      sentence = e['sentence']\n",
        "      sentence_list.append(sentence)\n",
        "  politician_sent.append(sentence_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dm3UXYuFzRYX",
        "outputId": "90153804-6a7d-4495-92d3-e9ed61efd806"
      },
      "source": [
        "len(actors_sent), len(cricketers_sent),len(politician_sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(34, 13, 27)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4d1jaya-UxG",
        "outputId": "7394443a-ff65-41e3-c248-049bed7765cc"
      },
      "source": [
        "len(actors_trip), len(cricketers_trip),len(politician_trip)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(34, 13, 27)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0ryOYvHtK0s"
      },
      "source": [
        "So, the sentence list is a list of lists--- each list containing sentences for 1 article/ entity id. Similarly, each triple list is a list of lists--- each list containing triples for 1 article/ entity id. \n",
        "Now , we run our algo on this to find the matcheing sentences and triples within each article. We keep it in a dictionary : key = sentence and val = matching list of tripes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGywhFm0puEA"
      },
      "source": [
        "def matches(sentences,triples):\n",
        "  tgworddict = {}\n",
        "  for sent in sentences:\n",
        "    srcwordlist = sent.split(' ')\n",
        "    tgwordlists = []\n",
        "    for src_word in srcwordlist:\n",
        "      # print(src_word)\n",
        "      src_word = src_word.translate(str.maketrans('', '', string.punctuation))  # Removing punct\n",
        "      tgtwords = get_nn(src_word, src_embeddings, src_id2word, tgt_embeddings, tgt_id2word, K=5)\n",
        "      if len(tgtwords)>1 and tgtwords!=None:\n",
        "        tgwordlists.extend(tgtwords)\n",
        "    tgworddict[sent] = tgwordlists\n",
        "\n",
        "  #Matches\n",
        "  matches_with_sent = {}\n",
        "  for sent in tgworddict:\n",
        "    words = tgworddict[sent]\n",
        "    matches = []\n",
        "    for entity in triples:\n",
        "      predicate = entity[1]\n",
        "      obj = entity[2]\n",
        "      score_predicate = wordoverlap(predicate,words)\n",
        "      score_obj = wordoverlap(obj,words)\n",
        "      if score_predicate >0 or score_obj >0:\n",
        "        # print(predicate,\"-\",obj,\"-\",words)\n",
        "        matches.append(entity)\n",
        "    if len(matches) >0:\n",
        "      #   # Sorting \n",
        "      # matches = np.asarray(matches)\n",
        "      # matches = matches[matches[:,1].argsort()][::-1]\n",
        "      # matches_with_sent[sent] = matches[:,0]\n",
        "      # matches = set(matches)\n",
        "      matches_with_sent[sent] = set(matches)\n",
        "  return matches_with_sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "To-jE963p6UD"
      },
      "source": [
        "matches_act = {}\n",
        "for sent_list,triple_list in zip(actors_sent,actors_trip):\n",
        "  for k,v in matches(sent_list,triple_list).items():\n",
        "    matches_act[k] = v\n",
        "\n",
        "matches_cric = {}\n",
        "for sent_list,triple_list in zip(cricketers_sent,cricketers_trip):\n",
        "  for k,v in matches(sent_list,triple_list).items():\n",
        "    matches_cric[k] = v\n",
        "\n",
        "matches_pol = {}\n",
        "for sent_list,triple_list in zip(politician_sent,politician_trip):\n",
        "  for k,v in matches(sent_list,triple_list).items():\n",
        "    matches_pol[k] = v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLawBBWUyLdP"
      },
      "source": [
        "### Evaluation : Precision and Recall"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6yeexNZSdTw"
      },
      "source": [
        "def evaluate(test_dict,matches_dict):\n",
        "  sum_prec = 0\n",
        "  for key,val in matches_dict.items():\n",
        "    tp,fp = 0,0\n",
        "    for k,v in test_dict.items():\n",
        "      #If sentence matches\n",
        "      if k == key:\n",
        "        for ent in v:\n",
        "          for trip in val:\n",
        "            if ent == trip:\n",
        "              tp = tp + 1\n",
        "        for trip in val:\n",
        "          flag = 0\n",
        "          for ent in v:\n",
        "            if ent == trip:\n",
        "              flag = 1\n",
        "              break\n",
        "          if flag == 0:\n",
        "            fp = fp +1\n",
        "        break\n",
        "    if (tp+fp)!=0:\n",
        "      prec = tp/(tp + fp)\n",
        "    else:\n",
        "      prec = 0\n",
        "    sum_prec = prec + sum_prec\n",
        "\n",
        "  sum_rec = 0\n",
        "  for k,v in test_dict.items():\n",
        "    rec = 0\n",
        "    tp,fp = 0,0\n",
        "    for key,val in matches_dict.items():\n",
        "      #If sentence matches\n",
        "      if k == key:\n",
        "        for ent in v:\n",
        "          for trip in val:\n",
        "            if ent == trip:\n",
        "              tp = tp + 1\n",
        "        for trip in val:\n",
        "          flag = 0\n",
        "          for ent in v:\n",
        "            if ent == trip:\n",
        "              flag = 1\n",
        "              break\n",
        "          if flag == 0:\n",
        "            fp = fp +1\n",
        "        break\n",
        "    rec = tp/len(v)\n",
        "    sum_rec = rec + sum_rec\n",
        "\n",
        "\n",
        "  avg_rec,avg_prec = sum_rec/len(test_dict),sum_prec/len(matches_dict)\n",
        "  return avg_rec, avg_prec\n",
        "\n",
        "\n",
        "avg_rec_act, avg_prec_act = evaluate(actors_test_dict,matches_act)\n",
        "avg_rec_cric, avg_prec_cric = evaluate(cricketers_test_dict,matches_cric)\n",
        "avg_rec_pol, avg_prec_pol = evaluate(politicians_test_dict,matches_pol)      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYhBXtxFJ5Ay",
        "outputId": "8c230b8f-b305-4c02-ef9e-9f1f17cbd3e6"
      },
      "source": [
        "(avg_rec_act,avg_prec_act), (avg_rec_cric, avg_prec_cric) , (avg_rec_pol, avg_prec_pol)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((0.8252777777777779, 0.431624966370729),\n",
              " (0.8205128205128205, 0.3855921855921856),\n",
              " (0.6055555555555556, 0.37839506172839504))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1VNWXqdNryR",
        "outputId": "cd186f66-e166-4b28-9bef-050aa9f92962"
      },
      "source": [
        "AverageRecall = (avg_rec_act + avg_rec_cric + avg_rec_pol)/3\n",
        "AveragePrecision = (avg_prec_act + avg_prec_cric + avg_prec_pol)/3\n",
        "\n",
        "AverageRecall, AveragePrecision"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7504487179487179, 0.3985374045637699)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bp0EwwYazDFK"
      },
      "source": [
        "for sent in actors_test_dict:\n",
        "  actors_test_dict[sent] = list(actors_test_dict[sent])\n",
        "for sent in cricketers_test_dict:\n",
        "  cricketers_test_dict[sent] = list(cricketers_test_dict[sent])\n",
        "for sent in politicians_test_dict:\n",
        "  politicians_test_dict[sent] = list(politicians_test_dict[sent])\n",
        "\n",
        "for sent in matches_act:\n",
        "  matches_act[sent] = list(matches_act[sent])\n",
        "for sent in matches_cric:\n",
        "  matches_cric[sent] = list(matches_cric[sent])\n",
        "for sent in matches_pol:\n",
        "  matches_pol[sent] = list(matches_pol[sent])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDdKaP_Szcs5"
      },
      "source": [
        "# saving the input test files\n",
        "with open(\"/content//drive//MyDrive//Word Overlap Output//actors_test.json\", \"w\") as outfile: \n",
        "    json.dump(actors_test_dict,outfile)\n",
        "with open(\"/content/drive/MyDrive/Word Overlap Output/cricketers_test.json\", \"w\") as outfile: \n",
        "    json.dump(cricketers_test_dict, outfile)\n",
        "with open(\"/content/drive/MyDrive/Word Overlap Output/politicians_test.json\", \"w\") as outfile: \n",
        "    json.dump(politicians_test_dict, outfile)\n",
        "\n",
        "#saving the output files\n",
        "with open(\"/content/drive/MyDrive/Word Overlap Output/actors_matches.json\", \"w\") as outfile: \n",
        "    json.dump(matches_act, outfile)\n",
        "with open(\"/content/drive/MyDrive/Word Overlap Output/cricketers_matches.json\", \"w\") as outfile: \n",
        "    json.dump(matches_cric, outfile)\n",
        "with open(\"/content/drive/MyDrive/Word Overlap Output/politicians_matches.json\", \"w\") as outfile: \n",
        "    json.dump(matches_pol, outfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K23E1awy_deb",
        "outputId": "6285ba8c-fa31-44b4-ee1d-e02f74759b29"
      },
      "source": [
        "matches_cric"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'अमय खुरासिया एक पूर्व भारतीय क्रिकेटर हैं।': {('Amay Khurasiya',\n",
              "   'country for sport',\n",
              "   'India'),\n",
              "  ('Amay Khurasiya', 'country of citizenship', 'India'),\n",
              "  ('Amay Khurasiya', 'member of sports team', 'India national cricket team'),\n",
              "  ('Amay Khurasiya', 'member of sports team', 'Madhya Pradesh cricket team'),\n",
              "  ('Amay Khurasiya', 'occupation', 'cricketer'),\n",
              "  ('Amay Khurasiya', 'sport', 'cricket')},\n",
              " 'कुमार श्री इंद्रजीत सिंहजी माधवसिंहजी (pronunciation सहायता·सूचना) (१५ जून १९३७ - १२ मार्च २०११) एक भारतीय क्रिकेट टीम के टेस्ट क्रिकेट खिलाड़ी थे ': {('Kumar Indrajitsinhji',\n",
              "   'CricketArchive player ID',\n",
              "   '1234'),\n",
              "  ('Kumar Indrajitsinhji', 'ESPNcricinfo.com player ID', '29533'),\n",
              "  ('Kumar Indrajitsinhji', 'country for sport', 'India'),\n",
              "  ('Kumar Indrajitsinhji', 'country of citizenship', 'British India'),\n",
              "  ('Kumar Indrajitsinhji', 'country of citizenship', 'Dominion of India'),\n",
              "  ('Kumar Indrajitsinhji', 'country of citizenship', 'India'),\n",
              "  ('Kumar Indrajitsinhji',\n",
              "   'member of sports team',\n",
              "   'India national cricket team'),\n",
              "  ('Kumar Indrajitsinhji', 'occupation', 'cricketer'),\n",
              "  ('Kumar Indrajitsinhji', 'sport', 'cricket')},\n",
              " 'कोट्टरी सुबन्ना नायडु भारतीय क्रिकेट खिलाड़ी थे, जिन्होंने 1934 से 1952 तक ग्यारह टेस्ट खेले।': {('C. S. Nayudu',\n",
              "   'CricketArchive player ID',\n",
              "   '634'),\n",
              "  ('C. S. Nayudu', 'ESPNcricinfo.com player ID', '31813'),\n",
              "  ('C. S. Nayudu', 'country for sport', 'India'),\n",
              "  ('C. S. Nayudu', 'country of citizenship', 'British India'),\n",
              "  ('C. S. Nayudu', 'country of citizenship', 'Dominion of India'),\n",
              "  ('C. S. Nayudu', 'country of citizenship', 'India'),\n",
              "  ('C. S. Nayudu', 'member of sports team', 'India national cricket team'),\n",
              "  ('C. S. Nayudu', 'occupation', 'cricketer'),\n",
              "  ('C. S. Nayudu', 'sport', 'cricket')},\n",
              " 'गैरी विल्सन (जन्म; ५ फरवरी १९८६, डुन्डोनाल्ड) आयरलैंड क्रिकेट टीम के वर्तमान कप्तान हैं।': {('Gary Wilson',\n",
              "   'country of citizenship',\n",
              "   'Ireland'),\n",
              "  ('Gary Wilson', 'date of birth', '1986-02-05T00:00:00Z'),\n",
              "  ('Gary Wilson', 'member of sports team', 'Derbyshire County Cricket Club'),\n",
              "  ('Gary Wilson', 'member of sports team', 'Ireland cricket team'),\n",
              "  ('Gary Wilson', 'member of sports team', 'Surrey County Cricket Club'),\n",
              "  ('Gary Wilson', 'occupation', 'cricketer'),\n",
              "  ('Gary Wilson', 'place of birth', 'Dundonald'),\n",
              "  ('Gary Wilson', 'sport', 'cricket')},\n",
              " 'जन्म २९ अप्रैल १९७४ को ऑस्ट्रेलिया मन हुआ था लेकिन खिलाड़ी आयरलैण्ड क्रिकेट टीम के बन गए। ': {('Trent Johnston',\n",
              "   'CricketArchive player ID',\n",
              "   '8178'),\n",
              "  ('Trent Johnston', 'ESPNcricinfo.com player ID', '6038'),\n",
              "  ('Trent Johnston', 'country of citizenship', 'Australia'),\n",
              "  ('Trent Johnston', 'date of birth', '1974-04-29T00:00:00Z'),\n",
              "  ('Trent Johnston', 'member of sports team', 'Ireland cricket team'),\n",
              "  ('Trent Johnston', 'member of sports team', 'New South Wales cricket team'),\n",
              "  ('Trent Johnston', 'occupation', 'cricketer'),\n",
              "  ('Trent Johnston', 'place of birth', 'Wollongong'),\n",
              "  ('Trent Johnston', 'sport', 'cricket')},\n",
              " 'नाउमल जिउमल माखीजा भारत के टेस्ट क्रिकेट के प्रथम ओपेनिंग बल्लेबाज थे।': {('Naoomal Jeoomal',\n",
              "   'country for sport',\n",
              "   'India'),\n",
              "  ('Naoomal Jeoomal', 'country of citizenship', 'British India'),\n",
              "  ('Naoomal Jeoomal', 'country of citizenship', 'Dominion of India'),\n",
              "  ('Naoomal Jeoomal', 'country of citizenship', 'India'),\n",
              "  ('Naoomal Jeoomal', 'member of sports team', 'India national cricket team'),\n",
              "  ('Naoomal Jeoomal', 'occupation', 'cricketer'),\n",
              "  ('Naoomal Jeoomal', 'sport', 'cricket')},\n",
              " 'माइकल विलियम गैटिंग एक अंग्रेजी पूर्व क्रिकेटर हैं': {('Mike Gatting',\n",
              "   'award received',\n",
              "   'Wisden Cricketer of the Year'),\n",
              "  ('Mike Gatting', 'member of sports team', 'England and Wales cricket team'),\n",
              "  ('Mike Gatting', 'member of sports team', 'Marylebone Cricket Club'),\n",
              "  ('Mike Gatting', 'member of sports team', 'Middlesex County Cricket Club'),\n",
              "  ('Mike Gatting', 'occupation', 'cricketer'),\n",
              "  ('Mike Gatting', 'sport', 'cricket')},\n",
              " 'माइकल हेंड्रिक (जन्म 22 अक्टूबर 1948)[1] एक पूर्व अंग्रेजी क्रिकेटर हैं, जो 1973 से 1981 तक इंग्लैंड के लिए तीस टेस्ट और बाईस एकदिवसीय मैच खेले।': {('Mike Hendrick',\n",
              "   'award received',\n",
              "   'Wisden Cricketer of the Year'),\n",
              "  ('Mike Hendrick', 'date of birth', '1948-10-22T00:00:00Z'),\n",
              "  ('Mike Hendrick', 'member of sports team', 'Derbyshire County Cricket Club'),\n",
              "  ('Mike Hendrick', 'member of sports team', 'England and Wales cricket team'),\n",
              "  ('Mike Hendrick',\n",
              "   'member of sports team',\n",
              "   'Nottinghamshire County Cricket Club'),\n",
              "  ('Mike Hendrick', 'occupation', 'cricketer'),\n",
              "  ('Mike Hendrick', 'place of birth', 'Darley Dale'),\n",
              "  ('Mike Hendrick', 'sport', 'cricket')},\n",
              " 'राजेश चौहान एक पूर्व भारतीय क्रिकेट खिलाड़ी है। ': {('Rajesh Chauhan',\n",
              "   'CricketArchive player ID',\n",
              "   '2071'),\n",
              "  ('Rajesh Chauhan', 'ESPNcricinfo.com player ID', '27622'),\n",
              "  ('Rajesh Chauhan', 'country for sport', 'India'),\n",
              "  ('Rajesh Chauhan', 'country of citizenship', 'India'),\n",
              "  ('Rajesh Chauhan', 'given name', 'Rajesh'),\n",
              "  ('Rajesh Chauhan', 'occupation', 'cricketer'),\n",
              "  ('Rajesh Chauhan', 'sport', 'cricket')},\n",
              " 'रामचंद्र सुधाकर राव एक पूर्व भारतीय क्रिकेट खिलाड़ी है जो भारत की राष्ट्रीय क्रिकेट टीम के लिए खेलते थे।': {('Sudhakar Rao',\n",
              "   'CricketArchive player ID',\n",
              "   '1483'),\n",
              "  ('Sudhakar Rao', 'ESPNcricinfo.com player ID', '34136'),\n",
              "  ('Sudhakar Rao', 'country for sport', 'India'),\n",
              "  ('Sudhakar Rao', 'country of citizenship', 'India'),\n",
              "  ('Sudhakar Rao', 'occupation', 'cricketer'),\n",
              "  ('Sudhakar Rao', 'sport', 'cricket')},\n",
              " 'राहुल शर्मा एक भारतीय क्रिकेट खिलाड़ी है ': {('Rahul Sharma',\n",
              "   'ESPNcricinfo.com player ID',\n",
              "   '272994'),\n",
              "  ('Rahul Sharma', 'country for sport', 'India'),\n",
              "  ('Rahul Sharma', 'country of citizenship', 'India'),\n",
              "  ('Rahul Sharma', 'family name', 'Sharma'),\n",
              "  ('Rahul Sharma', 'given name', 'Rahul'),\n",
              "  ('Rahul Sharma', 'member of sports team', 'India national cricket team'),\n",
              "  ('Rahul Sharma', 'member of sports team', 'Pune Warriors India'),\n",
              "  ('Rahul Sharma', 'occupation', 'cricketer'),\n",
              "  ('Rahul Sharma', 'sport', 'cricket')},\n",
              " 'राहुल संघवी एक पूर्व भारतीय क्रिकेटर है, जो बाएं हाथ के ऑर्थोडॉक्स स्पिन गेंदबाज है।': {('Rahul Sanghvi',\n",
              "   'country for sport',\n",
              "   'India'),\n",
              "  ('Rahul Sanghvi', 'country of citizenship', 'India'),\n",
              "  ('Rahul Sanghvi', 'given name', 'Rahul'),\n",
              "  ('Rahul Sanghvi', 'occupation', 'cricketer'),\n",
              "  ('Rahul Sanghvi', 'sport', 'cricket')},\n",
              " 'विलियम हॉवर्ड फ्रिंडाल एक अंग्रेजी क्रिकेट स्कोरर और सांख्यिकीविद थे|': {('Bill Frindall',\n",
              "   'member of sports team',\n",
              "   'Marylebone Cricket Club'),\n",
              "  ('Bill Frindall', 'occupation', 'cricketer'),\n",
              "  ('Bill Frindall', 'sport', 'cricket')}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ryql59cSgin"
      },
      "source": [
        "### Running a unit test on evaluate\n",
        "a ,b,c,d= 1,2,3,4\n",
        "test_dict = {'ABCD' : {(a,b,c),(a,b,d),(b,c,d)}, 'ABDC': {(a,b,c),(c,d,e)}, 'AABC' :{(a,b,c)}}\n",
        "matches_dict = {'ABCD' : {(a,b,c),(a,b,d)}, 'ABDC': {(a,b,c),(a,b,d)} }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuJgwKl_X5RJ"
      },
      "source": [
        "rec,prec = evaluate(test_dict,matches_dict)\n",
        "rec,prec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHNMZMJUoFsZ"
      },
      "source": [
        "(2/3 + 1/2 + 0)/3 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAIDZHZb8Bts"
      },
      "source": [
        "#### DIsplaying the matching sentences with the triples for the article about the actress - 'Kalpana'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N27fETqGS6pq"
      },
      "source": [
        "for k,v in matches_with_sent.items():\n",
        "  print(\"\\n\")\n",
        "  print(\"*******************************************************************************\")\n",
        "  print(k)\n",
        "  print(\"===============================\")\n",
        "  print(\"\\t\\t\\t\\t\\t\\n\",v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_QqaFAR84Yy"
      },
      "source": [
        "### Conclusion :\n",
        "The result we get is fairly good. For a lot of sentences , we get triples that are relevant. Because we are using google translate to get english transliteration for the out of vocab hindi words, the result is even better. "
      ]
    }
  ]
}